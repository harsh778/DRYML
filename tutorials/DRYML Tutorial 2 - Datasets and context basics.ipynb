{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa0e47f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T16:49:20.543612Z",
     "start_time": "2022-09-23T16:49:20.539999Z"
    }
   },
   "outputs": [],
   "source": [
    "import dryml\n",
    "from dryml.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e981ac3",
   "metadata": {},
   "source": [
    "# DRYML Tutorial 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552e21e",
   "metadata": {},
   "source": [
    "## DRYML `Dataset` and `context` basics\n",
    "\n",
    "Modern ML platforms suffer from two fairly annoying issues. First, iterating through datasets is in no way standardized. Each platform offers its own version of dataset ingestion, batching, and iterators. DRYML attempts to remedy this by offering a wrapper class called `Dataset` which implements a uniform set of functionality for two major ML platforms, as well as generic numpy datasets. Secondly, use of compute resources is often difficult to configure. By default, most platforms just allocate an entire GPU. DRYML attempts to remedy this in two ways. First, it provides a primative compute `context` system where methods requiring compute resources can request them. `context` is then able to launch a python process to contain compute operations. This allows device memory to be released when the method completes. Secondly, `Object` supports a 'compute' mode, where the user should specify any objects which require memory allocation on a device such as a GPU.\n",
    "\n",
    "We'll go over basic functionality of each of these components in this Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d48bca",
   "metadata": {},
   "source": [
    "## DRYML `Dataset` basics and `NumpyDataset`\n",
    "\n",
    "The `Dataset` API is an attempt to standardize ML dataset interaction. It takes a functional-style approach, borrowing much from `tensorflow`'s `tf.data.Dataset` type. We'll explore the `Dataset` type using a generated data sample loading into `NumpyDataset` wrapper. The `NumpyDataset` wrapper implements most of the `Dataset` API, and allows easy use of the data as well as common ML operations on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee93c9e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T17:25:00.422326Z",
     "start_time": "2022-09-23T17:25:00.420347Z"
    }
   },
   "source": [
    "### Creating `NumpyDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4e8222c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T16:57:40.675436Z",
     "start_time": "2022-09-23T16:57:40.673297Z"
    }
   },
   "outputs": [],
   "source": [
    "from dryml.data import NumpyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296d174e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T16:49:37.863645Z",
     "start_time": "2022-09-23T16:49:37.656750Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create random numpy dataset. \n",
    "num_examples = 10000\n",
    "data_shape = (28, 28)\n",
    "data_np = np.random.random((num_examples,)+data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e287020",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T16:58:23.420120Z",
     "start_time": "2022-09-23T16:58:23.416499Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create NumpyDataset from the numpy dataset supervised=False is necessary as we don't have supervised targets\n",
    "# in this dataset\n",
    "data_ds = NumpyDataset(data_np, supervised=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c147520",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T17:25:15.782153Z",
     "start_time": "2022-09-23T17:25:15.780118Z"
    }
   },
   "source": [
    "### `Dataset.peek`\n",
    "\n",
    "Now we can take advantage of the `Dataset` API. First, We'll introduce the very useful method `peek`. `peek` simply returns the first element of the `Dataset`. If the data is batched, it'll return the first batch, if not batched, it'll return the first element. Let's verify the shape of the first element of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "055e0f8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T16:58:30.375410Z",
     "start_time": "2022-09-23T16:58:30.371162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ds.peek().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6d858d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T17:25:15.782153Z",
     "start_time": "2022-09-23T17:25:15.780118Z"
    }
   },
   "source": [
    "### `Dataset.batch` and `Dataset.unbatch`\n",
    "\n",
    "Great! That's what we put into the dataset!, So what if we don't want to treat the entire dataset as one giant batch? Well we have the `unbatch` and `batch` methods. We can first `unbatch` the dataset, then `batch` it with the appropriate `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51c32f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T17:17:20.957205Z",
     "start_time": "2022-09-23T17:17:20.954913Z"
    }
   },
   "outputs": [],
   "source": [
    "# Unbatch, then rebatch with new batch size\n",
    "batch_size = 32\n",
    "batched_ds = data_ds.unbatch().batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e71ee51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T17:17:27.853937Z",
     "start_time": "2022-09-23T17:17:27.849730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 28, 28)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek at the new dataset object's element shape.\n",
    "batched_ds.peek().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c1a23",
   "metadata": {},
   "source": [
    "We now see the dataset gives batches with a `batch_size` of 32! Well, what if we want to look at one single example? Well, we just don't use the last `batch` call! And notice, the result of each method call is a new `Dataset` object! This means, we can re-use each as often as we like, and changes to the dataset don't destroy the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d778302",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T17:24:16.835018Z",
     "start_time": "2022-09-23T17:24:16.831170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ds.unbatch().peek().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e101d30",
   "metadata": {},
   "source": [
    "So we see, the shape of a single example is what we intended at the start!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718d5e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T17:27:52.542741Z",
     "start_time": "2022-09-23T17:27:52.540789Z"
    }
   },
   "source": [
    "### `Dataset.take` and `Dataset.skip`\n",
    "\n",
    "`Dataset` implements the `take` and `skip` methods as well. This is very useful for getting a limited set of data to interact with if the dataset is infinite or just very large. `Dataset` also provides the method `count` which attempts to literally count the elements in the `Dataset`. (This is in lieu of a better heuristic method we are working on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "811f4d4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T17:38:49.125767Z",
     "start_time": "2022-09-23T17:38:49.122256Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ds.unbatch().take(10).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0897f",
   "metadata": {},
   "source": [
    "### `Dataset` iteration\n",
    "\n",
    "`Dataset`s are python iterables! Let's have a look at that now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77787736",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T19:02:31.033418Z",
     "start_time": "2022-09-23T19:02:31.030379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6099607496776515\n",
      "0.8018996842937309\n",
      "0.8826141329289703\n",
      "0.7928764569998471\n",
      "0.3964297929012688\n",
      "0.9348840216061983\n",
      "0.7806513078157304\n",
      "0.4423940369166266\n",
      "0.9003693292511876\n",
      "0.9527959674923151\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the unbatched data and show the 0,0'th element\n",
    "for el in data_ds.unbatch().take(10):\n",
    "    print(el[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a84534",
   "metadata": {},
   "source": [
    "### `Dataset` - Supervised, Unsupervised, and Indexed datasets\n",
    "\n",
    "`Dataset` supports various types of datasets currently, the most common difference between two `Dataset`s is whether they are supervised, and whether they are indexed. Let's consider non-indexed `Dataset`s first. If such a `Dataset` is unsupervised, then each element will just be the `X` value (the value we would pass to a model.). If supervised, then each element will be a tuple `(X, Y)` where `Y` are the known labels for the supervised dataset. If the dataset is batched, then `X` and `Y` will be batches as well. If the dataset is indexed, the index data for each element will appear as the first element in the tuple. For unsupervised data, it will look like this: `(I, X)` where `I` is the index for the element. And for supervised data, it will look like this: `(I, (X, Y))`. Thus to get the index for indexed data, we access `el[0]` where `el` is the data element.\n",
    "\n",
    "Let's have a look at how this works for a supervised dataset. We'll create a new supervised dataset by generating random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "217140d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T20:18:00.860309Z",
     "start_time": "2022-09-23T20:18:00.831661Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the new supervised data\n",
    "num_examples = 10000\n",
    "x_shape = (20, 5)\n",
    "num_classes = 5\n",
    "x_data = np.random.random((num_examples,)+x_shape)\n",
    "y_data = np.random.choice(list(range(num_classes)), size=(num_examples,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4bca635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T20:25:48.643065Z",
     "start_time": "2022-09-23T20:25:48.640540Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create NumpyDataset\n",
    "ds = NumpyDataset((x_data, y_data), supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333f032",
   "metadata": {},
   "source": [
    "We can verify this data has the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d264220",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T20:42:05.772757Z",
     "start_time": "2022-09-23T20:42:05.768338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20, 5)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X shape: {ds.peek()[0].shape}\")\n",
    "print(f\"Y shape: {ds.peek()[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5859cc4c",
   "metadata": {},
   "source": [
    "We can index this dataset using the `as_indexed` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b228c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T20:44:40.854776Z",
     "start_time": "2022-09-23T20:44:40.849828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I shape: (10000,)\n",
      "X shape: (10000, 20, 5)\n",
      "Y shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "indexed_ds = ds.as_indexed()\n",
    "first_el = indexed_ds.peek()\n",
    "print(f\"I shape: {first_el[0].shape}\")\n",
    "print(f\"X shape: {first_el[1][0].shape}\")\n",
    "print(f\"Y shape: {first_el[1][1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba11b0",
   "metadata": {},
   "source": [
    "By default, `as_indexed` counts examples starting from 0, so we can look at the first 10 elements index of the data using `unbatch`, `take`, and the `index` method which returns an iterable which just gives the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c7197b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T20:50:24.294001Z",
     "start_time": "2022-09-23T20:50:24.289940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for el in indexed_ds.unbatch().take(10).index():\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da8ecee",
   "metadata": {},
   "source": [
    "We can also remove the supervised data from `ds` using the `as_unsupervised` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e96bd8a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T20:57:54.408990Z",
     "start_time": "2022-09-23T20:57:54.405283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 20, 5)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.as_not_supervised().peek().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4503ea66",
   "metadata": {},
   "source": [
    "### `Dataset` - `map`, `map_el`, `apply`, `apply_X`, and `apply_Y`\n",
    "\n",
    "`Dataset` supports mapping of a function across all elements of the dataset. This is useful for applying transformations to the dataset, and other components of DRYML operating on datasets use these methods in their implementations.\n",
    "\n",
    "* `map` applies a given function to all content in the `Dataset`.\n",
    "* `apply` applies a given function to all `X` and `Y` content in the `Dataset`.\n",
    "* `apply_X` applies a given function only to the `X` dataset in a `Dataset`.\n",
    "* `apply_Y` applies a given function only to the `Y` dataset in a `Dataset`.\n",
    "* `map_el` is a special function. You should have noticed at this point that all elements yielded by the dataset are within a tuple or by themselves. So `map_el` applies a function to each piece of primitive data within a collection such as tuple and applies a given function to every primative element of data within supported collections. So if the element is `(d1, (d2, d3))`, this will give `(f(d1), (f(d2), f(d3))`.\n",
    "\n",
    "Let's try `apply_X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "afcd5be4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T21:09:14.945663Z",
     "start_time": "2022-09-23T21:09:14.938046Z"
    }
   },
   "outputs": [],
   "source": [
    "# We apply a function to the Dataset\n",
    "el_sq = ds.apply_X(lambda x: x**2).peek()[0]\n",
    "# We can check that the function was applied with an assert\n",
    "el = ds.peek()[0]\n",
    "assert np.all(el_sq == el**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f760f",
   "metadata": {},
   "source": [
    "## DRYML context basics\n",
    "\n",
    "DRYML implements 'compute contexts' for specific ML frameworks. Resources for these contexts can be requested for each of these contexts using the keyword for each. We have the following keywords: for tensorflow 'tf', for pytorch 'torch', and for default 'default'. We can then build up a dictionary of 'resource requests' like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df35556a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T15:00:08.310723Z",
     "start_time": "2022-09-23T15:00:08.307445Z"
    }
   },
   "outputs": [],
   "source": [
    "ctx_reqs = {\n",
    "    'default': {'num_gpus': 0},\n",
    "    'tf': {'num_gpus': 1},\n",
    "    'torch': {'num_gpus': 0},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59599b13",
   "metadata": {},
   "source": [
    "A resource request is a dictionary with a couple keys to signal a request for specific resources. Right now we can ask for a specific number of cpus/gpus with `num_cpus` and `num_gpus`. We can also ask for specific cpus and gpus with `cpu/<i>` and `gpu/<i>` with a float value between `0.` and `1.`. When possible, if you request a fraction of a gpu, DRYML will configure the corresponding framework for that.\n",
    "\n",
    "Thus, the above context requirements asks for tensorflow with one gpu, and torch with no gpus.\n",
    "\n",
    "With the `ctx_reqs` dictionary, DRYML will create a `ContextManager` which will attempt to create appropriate contexts with the correct resources. If successful, the user will have access to the necessary GPUs, and the correponding libraries will be configured for the requested devices (if possible).\n",
    "\n",
    "> Be aware that most frameworks currently have no way of enforcing limits on memory consumption of GPUs. This means, the user is trusted to try and adhere to the memory requirements which DRYML makes available at all times through the `dryml.get_context()` method which returns the current `ContextManager`.\n",
    "\n",
    "If the user wants their objects to avoid allocating memory on a device, they can simply not set a context, and if a context is required, DRYML will throw an exception.\n",
    "\n",
    "For the rest of this Tutorial, we'll set the compute context using the resources above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d10f321",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T15:13:00.085338Z",
     "start_time": "2022-09-23T15:12:55.410949Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 10:13:00.080735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5780 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "dryml.context.set_context(ctx_reqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237265ea",
   "metadata": {},
   "source": [
    "If there is code you suspect may require device memory, DRYML provides the `context_check` method to check whether the current context satisfies some resource constraints. Let's check if the current context has two GPUs allocated to tensorflow. (This should fail!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3233096b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T15:19:32.526770Z",
     "start_time": "2022-09-23T15:19:32.505540Z"
    }
   },
   "outputs": [
    {
     "ename": "ContextIncompatibilityError",
     "evalue": "Context doesn't satisfy requirements {'tf': {'num_gpus': 2}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mContextIncompatibilityError\u001b[0m               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdryml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_gpus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data0/matthew/Software/NCSA/DRYML/src/dryml/context/context_tracker.py:404\u001b[0m, in \u001b[0;36mcontext_check\u001b[0;34m(ctx_reqs)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoContextError()\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ctx_mgr\u001b[38;5;241m.\u001b[39msatisfies(ctx_reqs):\n\u001b[0;32m--> 404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContextIncompatibilityError(\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt satisfy requirements \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mctx_reqs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mContextIncompatibilityError\u001b[0m: Context doesn't satisfy requirements {'tf': {'num_gpus': 2}}"
     ]
    }
   ],
   "source": [
    "dryml.context.context_check({'tf': {'num_gpus': 2}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52153432",
   "metadata": {},
   "source": [
    "And we'll just double check that the current context satisfies the requirements we set out earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4783bbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T15:22:05.721063Z",
     "start_time": "2022-09-23T15:22:05.717136Z"
    }
   },
   "outputs": [],
   "source": [
    "dryml.context.context_check(ctx_reqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e146b",
   "metadata": {},
   "source": [
    "## DRYML `Dataset` - ML Framework transformations\n",
    "\n",
    "DRYML `Dataset` comes with a special new power. `Datasets` can implement transformations to datasets in other ML frameworks! For example, tensorflow!. the `NumpyDataset` class supports the method `tf` which creates a new `tf.data.Dataset` wrapped in a `TFDataset` which contains the data from the original `NumpyDataset`! This is very useful for moving data into tensorflow tensors, to be used in tensorflow models. Since we now have a context which supports tensorflow, we can proceed with this transformation.\n",
    "\n",
    "> This type of transformation or creation of `tf.data.Dataset`s is one good use for the `context_check` method we just saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4950e91a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T21:16:56.688242Z",
     "start_time": "2022-09-23T21:16:56.633111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_ds = ds.tf()\n",
    "# We can now peek at the first element of the new dataset and see it's type.\n",
    "type(tf_ds.peek()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f0b5a",
   "metadata": {},
   "source": [
    "We can see it's now a tensorflow `EagerTensor` (it may also be just a `Tensor`).\n",
    "\n",
    "Other transformations may exist as well. the `torch` method turns the tensor into a pytorch tensor, and `numpy` turns it back into a `NumpyDataset`. Be aware, that these types of transformations currently come with large performance hits, and there is a benefit to staying within a single ML ecosystem, however this ability makes exploring new algorithms much simpler, as we don't have to re-program our data source right away and can take advantage of data input pipelines already built in other frameworks when testing new frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b4f6f73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T21:23:03.549569Z",
     "start_time": "2022-09-23T21:23:02.627235Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data0/matthew/Software/NCSA/DRYML/venv_dryml_dev/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for pytorch\n",
    "type(ds.torch().peek()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "17f16e74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T21:24:20.917895Z",
     "start_time": "2022-09-23T21:24:20.864654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can go back to numpy!\n",
    "type(ds.tf().numpy().peek()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095ed18",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "Like other components of DRYML, `Dataset`s and `dryml.context` can be used outside of `DRYML`. `dryml.context` is very useful for automatically setting ML framework's device settings. and `Dataset` is great for inspecting and bridging existing datasets between different frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6833217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:venv_dryml_dev]",
   "language": "python",
   "name": "conda-env-venv_dryml_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
