{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e473b2bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:48:03.262855Z",
     "start_time": "2022-09-28T15:48:03.093971Z"
    }
   },
   "outputs": [],
   "source": [
    "import dryml\n",
    "from dryml import ObjectDef\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435ab16b",
   "metadata": {},
   "source": [
    "# DRYML Tutorial 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de1ae77",
   "metadata": {},
   "source": [
    "## DRYML Contexts 2\n",
    "\n",
    "We've had some experience with contexts, but now we'll discuss how we can avoid creating a context in our current process. This allows us to possibly change how resources are distributed depending on the model. The `dryml.compute_context` decorator generator is provided by DRYML which gives a wrapped method the power to inspect existing compute contexts, or launch itself in a new process with an appropriate context. This makes it easy to interleave code requiring a context with manager code which may require running a variety of models that could have conflicting context requirements.\n",
    "\n",
    "Now, `dryml.compute_context` is actually a decorator generator meaning, you need to call it to create the decorator you want to use. This allows the user to customize how a given method gets wrapped, and customizes how compute contexts are spawned. DRYML also provides the `dryml.compute` decorator which is just a shortcut to `compute_context()` when generic behavior is fine.\n",
    "\n",
    "`compute_context` has a couple of important arguments which can be specified when the decorator is created (when calling `compute_context`), and can be overridden when actually calling the function.\n",
    "* `ctx_context_reqs`: Probably the most important, specifies a specific set of `context_reqs` to use when checking for an existing context or launching a new context. Override at call time with `call_context_reqs`.\n",
    "* `ctx_use_existing_context` (Default `True`): When `True`, DRYML should try to use an existing context if available. If the existing context doesn't satisfy the given requirements, it will raise a `WrongContextError` exception rather than create a new context. Override at call time with `call_use_existing_context`\n",
    "* `ctx_dont_create_context` (Default `False`): When `False`, DRYML won't try to create a new context ever. if no context exists, it'll throw a `NoContextError` exception, and if the existing context doesn't satisfy the given requirements, it will raise a `WrongContextError` exception. Override at call time with `call_dont_create_context`\n",
    "* `ctx_update_objs` (Default `False`): When `True`, DRYML will update objects in the current process with the state of corresponding objects in the remove process upon completion. Override at call time with `call_update_objs`\n",
    "* `ctx_verbose` (Default `False`): When `True`, DRYML will print some diagnostic information about the whole compute procedure. Override at call time with `call_verbose`.\n",
    "\n",
    "\n",
    "First, we'll create a function to create our mnist dataset which will first check if an appropriate context is in place. Then we'll write a function for training a model and we'll use the `compute_context` decorator to indicate that this method needs a compute context. Then we'll create three simple models using `sklearn`, `tensorflow`, and `pytorch`. We will then explore the statistical variations of these models by training multiple copies of them and measuring their accuracies.\n",
    "\n",
    "One complication we'll need to overcome, is that by default, DRYML uses the `spawn` method for creating new subprocesses. This means an entirely new python process is created, and any function definitions defined in this Jupyter notebook will not be available. Typically the way to solve this is to create a python module and place our needed functions in there, however we don't want our users to open an external editor just for the sake of one method. So we've come up with a workaround here you can take with you and use when needed in other contexts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "915d264c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:13:37.391357Z",
     "start_time": "2022-09-28T15:13:37.383778Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create function to generate datasets\n",
    "def gen_dataset():\n",
    "    # import some names\n",
    "    import dryml\n",
    "    import tensorflow_datasets as tfds\n",
    "    from dryml.data.tf import TFDataset\n",
    "\n",
    "    # Check that the context has tensorflow ability, but don't get specific.\n",
    "    dryml.context.context_check({'tf': {}})\n",
    "\n",
    "    (ds_train, ds_test), ds_info = tfds.load(\n",
    "        'mnist',\n",
    "        split=['train', 'test'],\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True)\n",
    "    \n",
    "    train_ds = TFDataset(\n",
    "        ds_train,\n",
    "        supervised=True,\n",
    "    )\n",
    "    \n",
    "    test_ds = TFDataset(\n",
    "        ds_test,\n",
    "        supervised=True,\n",
    "    )\n",
    "    \n",
    "    return train_ds, test_ds\n",
    "\n",
    "class temp_mod_file(object):\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.file = open(self.name, mode='w')\n",
    "\n",
    "    def write_obj_source(self, obj):\n",
    "        import inspect\n",
    "        obj_source = inspect.getsource(obj)\n",
    "        self.file.write(obj_source)\n",
    "        self.file.write(\"\\n\")\n",
    "        self.file.flush()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.file.close()\n",
    "        import os\n",
    "        os.remove(self.name)\n",
    "        del self.file\n",
    "\n",
    "# Delete original file\n",
    "try:\n",
    "    del mod_file\n",
    "except NameError:\n",
    "    pass\n",
    "mod_file = temp_mod_file('temp_mod.py')\n",
    "mod_file.write_obj_source(gen_dataset)\n",
    "del gen_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "402597ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:13:37.406198Z",
     "start_time": "2022-09-28T15:13:37.392998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create function to train a model.\n",
    "# We use ctx_update_objs=True to indicate any objects we give the method should be updated with their\n",
    "# state at the end of the method.\n",
    "@dryml.compute_context(ctx_update_objs=True)\n",
    "def train_model(model):\n",
    "    from temp_mod import gen_dataset\n",
    "    train_ds, _ = gen_dataset()\n",
    "    \n",
    "    model.prep_train()\n",
    "    model.train(train_ds)\n",
    "\n",
    "\n",
    "# Create function to test model\n",
    "# Since this method doesn't change the models, we don't have to update them after calling it.\n",
    "@dryml.compute\n",
    "def test_model(model):\n",
    "    from dryml.metrics import categorical_accuracy\n",
    "    from temp_mod import gen_dataset\n",
    "    _, test_ds = gen_dataset()\n",
    "    \n",
    "    model.prep_eval()\n",
    "    return categorical_accuracy(model, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f4dbee",
   "metadata": {},
   "source": [
    "## Create ML Models\n",
    "\n",
    "Now we'll create a few model classes using `ObjectDef`s. We'll then use `ObjectDef.build` to create instances of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8413fdde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:13:37.649601Z",
     "start_time": "2022-09-28T15:13:37.408080Z"
    }
   },
   "outputs": [],
   "source": [
    "import dryml.models\n",
    "import dryml.data\n",
    "import dryml.models.sklearn\n",
    "import sklearn.neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7699e096",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:13:37.654888Z",
     "start_time": "2022-09-28T15:13:37.651633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's define some common processing steps so we don't have to build full definitions for them every time.\n",
    "flatten_def = ObjectDef(dryml.data.transforms.Flatten)\n",
    "best_cat_def = ObjectDef(dryml.data.transforms.BestCat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33627cf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:13:37.670551Z",
     "start_time": "2022-09-28T15:13:37.656183Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, we'll build an sklearn model.\n",
    "sklearn_mdl_def = ObjectDef(\n",
    "    dryml.models.Pipe,\n",
    "    flatten_def,\n",
    "    ObjectDef(\n",
    "        dryml.models.sklearn.Trainable,\n",
    "        model=ObjectDef(\n",
    "            dryml.models.sklearn.ClassifierModel,\n",
    "            sklearn.neighbors.KNeighborsClassifier,\n",
    "            n_neighbors=10,\n",
    "        ),\n",
    "        train_fn=ObjectDef(\n",
    "            dryml.models.sklearn.BasicTraining,\n",
    "            num_examples=500,\n",
    "            shuffle=True,\n",
    "            shuffle_buffer_size=5000,\n",
    "        )\n",
    "    ),\n",
    "    best_cat_def,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c7cfd0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:13:46.291810Z",
     "start_time": "2022-09-28T15:13:37.671802Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 10:13:40.056233: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8205128205128205"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we can generate, train and test a model.\n",
    "simple_tf_reqs = {'tf': {}}\n",
    "temp_model = sklearn_mdl_def.build()\n",
    "train_model(temp_model, call_context_reqs=simple_tf_reqs)\n",
    "test_model(temp_model, call_context_reqs=simple_tf_reqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f442c660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:13:46.304125Z",
     "start_time": "2022-09-28T15:13:46.295809Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_multiple(model_def=None, num_to_train=None, ctx_reqs=None):\n",
    "    models = []\n",
    "    accuracies = []\n",
    "    for i in range(num_to_train):\n",
    "        new_model = model_def.build()\n",
    "        train_model(new_model, call_context_reqs=ctx_reqs)\n",
    "        acc = test_model(new_model, call_context_reqs=ctx_reqs)\n",
    "        accuracies.append(acc)\n",
    "        models.append(new_model)\n",
    "\n",
    "    return models, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f00198a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:13:46.316625Z",
     "start_time": "2022-09-28T15:13:46.307607Z"
    }
   },
   "outputs": [],
   "source": [
    "num_to_train = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bdae0d",
   "metadata": {},
   "source": [
    "Now, let's write a function which takes a definition, trains some number of models, tests them and returns the trained models as well as the mean accuracy and accuracy deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dc5980b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:14:28.951657Z",
     "start_time": "2022-09-28T15:13:46.323632Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 10:13:48.730414: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-09-28 10:13:57.274460: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-09-28 10:14:05.671212: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-09-28 10:14:14.399427: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-09-28 10:14:22.853404: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "sklearn_models, sklearn_accuracies = train_multiple(\n",
    "    model_def=sklearn_mdl_def,\n",
    "    num_to_train=num_to_train,\n",
    "    ctx_reqs={'tf': {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81aac925",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:14:28.959260Z",
     "start_time": "2022-09-28T15:14:28.954798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn accuracy: 0.8109575320512821+/-0.009176691219372033\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy mean/stddev\n",
    "print(f\"sklearn accuracy: {np.mean(sklearn_accuracies)}+/-{np.std(sklearn_accuracies)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c581eb96",
   "metadata": {},
   "source": [
    "Now, let's build a tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "046507a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:14:30.285545Z",
     "start_time": "2022-09-28T15:14:28.962032Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import dryml.models.tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a4a7111",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:14:30.292564Z",
     "start_time": "2022-09-28T15:14:30.287411Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl_def = ObjectDef(\n",
    "    dryml.models.tf.keras.SequentialFunctionalModel,\n",
    "    input_shape=(28, 28, 1),\n",
    "    layer_defs=[\n",
    "        ['Conv2D', {'filters': 16, 'kernel_size': 3, 'activation': 'relu'}],\n",
    "        ['Conv2D', {'filters': 16, 'kernel_size': 3, 'activation': 'relu'}],\n",
    "        ['Flatten', {}],\n",
    "        ['Dense', {'units': 10, 'activation': 'linear'}],\n",
    "    ]\n",
    ")\n",
    "tf_mdl_def = ObjectDef(\n",
    "    dryml.models.Pipe,\n",
    "    ObjectDef(\n",
    "        dryml.models.tf.keras.Trainable,\n",
    "        model=mdl_def,\n",
    "        train_fn=ObjectDef(\n",
    "            dryml.models.tf.keras.BasicTraining,\n",
    "            epochs=2\n",
    "        ),\n",
    "        optimizer=ObjectDef(\n",
    "            dryml.models.tf.ObjectWrapper,\n",
    "            tf.keras.optimizers.Adam,\n",
    "        ),\n",
    "        loss=ObjectDef(\n",
    "            dryml.models.tf.ObjectWrapper,\n",
    "            tf.keras.losses.SparseCategoricalCrossentropy,\n",
    "            obj_kwargs={\n",
    "                'from_logits': True,\n",
    "            }\n",
    "        )\n",
    "    ),\n",
    "    ObjectDef(\n",
    "        dryml.data.transforms.BestCat\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7fa3aba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:16:35.555982Z",
     "start_time": "2022-09-28T15:14:30.293847Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 10:14:32.557766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7369 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 10:14:39.641183: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3997 - val_loss: 0.1271\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0727 - val_loss: 0.1226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 10:14:52.909037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7369 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\n",
      "2022-09-28 10:14:57.836955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7369 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 10:15:04.720139: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 6s 3ms/step - loss: 0.3982 - val_loss: 0.0999\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0621 - val_loss: 0.0990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 10:15:18.332227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7369 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\n",
      "2022-09-28 10:15:23.170913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7369 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 10:15:29.976351: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3083 - val_loss: 0.0944\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0677 - val_loss: 0.0986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 10:15:42.584342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7369 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\n",
      "2022-09-28 10:15:47.596784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7369 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 10:15:54.816107: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.4342 - val_loss: 0.0985\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0743 - val_loss: 0.1209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 10:16:07.720902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7369 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\n",
      "2022-09-28 10:16:12.781477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7369 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 10:16:19.580021: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.4158 - val_loss: 0.1340\n",
      "Epoch 2/2\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0796 - val_loss: 0.1130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 10:16:32.670055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7369 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "tf_models, tf_accuracies = train_multiple(\n",
    "    model_def=tf_mdl_def,\n",
    "    num_to_train=num_to_train,\n",
    "    ctx_reqs={'tf': {'gpu/1': 1.}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac710920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:16:35.565959Z",
     "start_time": "2022-09-28T15:16:35.560017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf accuracy: 0.9724959935897436+/-0.0018658863817620085\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy mean/stddev\n",
    "print(f\"tf accuracy: {np.mean(tf_accuracies)}+/-{np.std(tf_accuracies)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066bd118",
   "metadata": {},
   "source": [
    "And now, let's have a look at a similar pytorch model, We'll have to add another step to change the order of the indicies of the data since pytorch expects data in nchw format while tensorflow uses nhwc format. We'll also have to add a `TorchDevice` transformation to make sure the data is on the cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e3503fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:16:36.098023Z",
     "start_time": "2022-09-28T15:16:35.568707Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data0/matthew/Software/NCSA/DRYML/venv_dryml_dev/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dryml.models.torch\n",
    "import dryml.data.torch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7eab36b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:16:36.105738Z",
     "start_time": "2022-09-28T15:16:36.099446Z"
    }
   },
   "outputs": [],
   "source": [
    "mdl_def = ObjectDef(\n",
    "    dryml.models.torch.generic.Sequential,\n",
    "    layer_defs=[\n",
    "        [torch.nn.LazyConv2d, (16, 3), {}],\n",
    "        [torch.nn.ReLU, (), {}],\n",
    "        [torch.nn.LazyConv2d, (16, 3), {}],\n",
    "        [torch.nn.ReLU, (), {}],\n",
    "        [torch.nn.Flatten, (), {}],\n",
    "        [torch.nn.LazyLinear, (10,), {}],\n",
    "    ]\n",
    ")\n",
    "torch_mdl_def = ObjectDef(\n",
    "    dryml.models.Pipe,\n",
    "    ObjectDef(\n",
    "        dryml.data.transforms.Transpose,\n",
    "        axes=(2, 0, 1)\n",
    "    ),\n",
    "    ObjectDef(\n",
    "        dryml.data.transforms.Cast,\n",
    "        dtype='float32'\n",
    "    ),\n",
    "    ObjectDef(\n",
    "        dryml.models.torch.generic.Trainable,\n",
    "        model=mdl_def,\n",
    "        train_fn=ObjectDef(\n",
    "            dryml.models.torch.generic.BasicTraining,\n",
    "            optimizer=ObjectDef(\n",
    "                dryml.models.torch.generic.TorchOptimizer,\n",
    "                torch.optim.Adam,\n",
    "                mdl_def,\n",
    "            ),\n",
    "            loss=ObjectDef(\n",
    "                dryml.models.torch.generic.TorchObject,\n",
    "                torch.nn.CrossEntropyLoss\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    ObjectDef(\n",
    "        dryml.data.torch.transforms.TorchDevice,\n",
    "        device='cpu'\n",
    "    ),\n",
    "    ObjectDef(\n",
    "        dryml.data.transforms.BestCat\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fc4437f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:19:52.461230Z",
     "start_time": "2022-09-28T15:16:36.107088Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data0/matthew/Software/NCSA/DRYML/venv_dryml_dev/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "100%|██████████| 1875/1875 [00:16<00:00, 111.09it/s, loss=0.00905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss: 0.009048364644496662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data0/matthew/Software/NCSA/DRYML/venv_dryml_dev/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "/data0/matthew/Software/NCSA/DRYML/venv_dryml_dev/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "100%|██████████| 1875/1875 [00:17<00:00, 109.30it/s, loss=0.00799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss: 0.007985602132917848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data0/matthew/Software/NCSA/DRYML/venv_dryml_dev/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "/data0/matthew/Software/NCSA/DRYML/venv_dryml_dev/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "100%|██████████| 1875/1875 [00:17<00:00, 108.86it/s, loss=0.00789]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss: 0.007888825566699962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/data0/matthew/Software/NCSA/DRYML/venv_dryml_dev/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "/data0/matthew/Software/NCSA/DRYML/venv_dryml_dev/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "100%|██████████| 1875/1875 [00:17<00:00, 109.16it/s, loss=0.00702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss: 0.007018250606326425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data0/matthew/Software/NCSA/DRYML/venv_dryml_dev/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "/data0/matthew/Software/NCSA/DRYML/venv_dryml_dev/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "100%|██████████| 1875/1875 [00:17<00:00, 109.60it/s, loss=0.00712]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss: 0.007122587668206446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/data0/matthew/Software/NCSA/DRYML/venv_dryml_dev/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "torch_models, torch_accuracies = train_multiple( model_def=torch_mdl_def,\n",
    "    num_to_train=num_to_train,\n",
    "    ctx_reqs={'tf': {}, 'torch': {'gpu/1': 1.}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b634c8ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-28T15:19:52.467139Z",
     "start_time": "2022-09-28T15:19:52.463190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch accuracy: 0.9678285256410255+/-0.002065730925720642\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy mean/stddev\n",
    "print(f\"torch accuracy: {np.mean(torch_accuracies)}+/-{np.std(torch_accuracies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e61256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:venv_dryml_dev]",
   "language": "python",
   "name": "conda-env-venv_dryml_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
