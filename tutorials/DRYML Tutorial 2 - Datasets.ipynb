{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0e47f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T16:49:20.543612Z",
     "start_time": "2022-09-23T16:49:20.539999Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:01.396713Z",
     "iopub.status.busy": "2023-03-20T21:42:01.395562Z",
     "iopub.status.idle": "2023-03-20T21:42:01.615460Z",
     "shell.execute_reply": "2023-03-20T21:42:01.614660Z",
     "shell.execute_reply.started": "2023-03-20T21:42:01.396577Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dryml\n",
    "from dryml.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552e21e",
   "metadata": {},
   "source": [
    "# DRYML Tutorial 2 - `Dataset`s\n",
    "\n",
    "Interacting with datasets is in no way standardized between the major ML platforms. Each platform offers its own version of dataset ingestion, batching, and iteration. DRYML attempts to remedy this by offering a wrapper class called `Dataset` which implements a uniform set of functionality for two major ML platforms (and in some cases some functionality none implement!).  Once a common API is defined, developers using DRYML can rely on it and create pipelines which can communicate with eachother with minimal effort.\n",
    "\n",
    "## `NumpyDataset`\n",
    "\n",
    "\n",
    "The `Dataset` API is an attempt to standardize ML dataset interaction. It takes a functional-style approach, borrowing much from `tensorflow`'s `tf.data.Dataset` type. We'll explore the `Dataset` type by generating a data sample using `numpy` and loading it into a `NumpyDataset` object. The `NumpyDataset` object implements the `Dataset` API using `numpy` arrays and operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e8222c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T16:57:40.675436Z",
     "start_time": "2022-09-23T16:57:40.673297Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:01.617742Z",
     "iopub.status.busy": "2023-03-20T21:42:01.617414Z",
     "iopub.status.idle": "2023-03-20T21:42:01.621067Z",
     "shell.execute_reply": "2023-03-20T21:42:01.620519Z",
     "shell.execute_reply.started": "2023-03-20T21:42:01.617720Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dryml.data import NumpyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "296d174e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T16:49:37.863645Z",
     "start_time": "2022-09-23T16:49:37.656750Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:01.621925Z",
     "iopub.status.busy": "2023-03-20T21:42:01.621750Z",
     "iopub.status.idle": "2023-03-20T21:42:01.691979Z",
     "shell.execute_reply": "2023-03-20T21:42:01.691338Z",
     "shell.execute_reply.started": "2023-03-20T21:42:01.621907Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create random numpy dataset. \n",
    "num_examples = 10000\n",
    "data_shape = (28, 28)\n",
    "data_np = np.random.random((num_examples,)+data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e287020",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T16:58:23.420120Z",
     "start_time": "2022-09-23T16:58:23.416499Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:01.695138Z",
     "iopub.status.busy": "2023-03-20T21:42:01.694974Z",
     "iopub.status.idle": "2023-03-20T21:42:01.698538Z",
     "shell.execute_reply": "2023-03-20T21:42:01.698017Z",
     "shell.execute_reply.started": "2023-03-20T21:42:01.695120Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create NumpyDataset from the numpy dataset supervised=False is necessary as we don't have supervised targets\n",
    "# in this dataset\n",
    "data_ds = NumpyDataset(data_np, supervised=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c147520",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T17:25:15.782153Z",
     "start_time": "2022-09-23T17:25:15.780118Z"
    }
   },
   "source": [
    "### `Dataset.peek`\n",
    "\n",
    "We'll introduce the very useful method `peek`. `peek` simply returns the first element of the `Dataset`. If the data is batched, it'll return the first batch, if not batched, it'll return the first element. Let's verify the shape of the first element of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "055e0f8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T16:58:30.375410Z",
     "start_time": "2022-09-23T16:58:30.371162Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:02.926743Z",
     "iopub.status.busy": "2023-03-20T21:42:02.926296Z",
     "iopub.status.idle": "2023-03-20T21:42:02.937253Z",
     "shell.execute_reply": "2023-03-20T21:42:02.936450Z",
     "shell.execute_reply.started": "2023-03-20T21:42:02.926717Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ds.peek().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6d858d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T17:25:15.782153Z",
     "start_time": "2022-09-23T17:25:15.780118Z"
    }
   },
   "source": [
    "### `Dataset.batch` and `Dataset.unbatch`\n",
    "\n",
    "Great! That's what we put into the dataset!, So what if we don't want to treat the entire dataset as one giant batch? Well we have the `unbatch` and `batch` methods. We can first `unbatch` the dataset, then `batch` it with the appropriate `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51c32f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T17:17:20.957205Z",
     "start_time": "2022-09-23T17:17:20.954913Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:04.158311Z",
     "iopub.status.busy": "2023-03-20T21:42:04.157734Z",
     "iopub.status.idle": "2023-03-20T21:42:04.164704Z",
     "shell.execute_reply": "2023-03-20T21:42:04.163438Z",
     "shell.execute_reply.started": "2023-03-20T21:42:04.158258Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unbatch, then rebatch with new batch size\n",
    "batch_size = 32\n",
    "batched_ds = data_ds.unbatch().batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e71ee51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T17:17:27.853937Z",
     "start_time": "2022-09-23T17:17:27.849730Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:04.630070Z",
     "iopub.status.busy": "2023-03-20T21:42:04.629636Z",
     "iopub.status.idle": "2023-03-20T21:42:04.636188Z",
     "shell.execute_reply": "2023-03-20T21:42:04.635582Z",
     "shell.execute_reply.started": "2023-03-20T21:42:04.630043Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek at the new dataset object's element shape.\n",
    "batched_ds.peek().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c1a23",
   "metadata": {},
   "source": [
    "We now see the dataset gives batches with a `batch_size` of 32! Well, what if we want to look at one single example? Well, we just don't use the last `batch` call! And notice, the result of each method call is a new `Dataset` object! This means, we can re-use each as often as we like, and changes to the dataset don't destroy the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d778302",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T17:24:16.835018Z",
     "start_time": "2022-09-23T17:24:16.831170Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:05.375936Z",
     "iopub.status.busy": "2023-03-20T21:42:05.375659Z",
     "iopub.status.idle": "2023-03-20T21:42:05.382449Z",
     "shell.execute_reply": "2023-03-20T21:42:05.381329Z",
     "shell.execute_reply.started": "2023-03-20T21:42:05.375914Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ds.unbatch().peek().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e101d30",
   "metadata": {},
   "source": [
    "So we see, the shape of a single example is what we intended at the start!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718d5e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T17:27:52.542741Z",
     "start_time": "2022-09-23T17:27:52.540789Z"
    }
   },
   "source": [
    "### `Dataset.take` and `Dataset.skip`\n",
    "\n",
    "Useful for getting a limited set of data to interact with if the dataset is infinite or just very large. `Dataset` also provides the method `count` which attempts to literally count the elements in the `Dataset`. (This is in lieu of a better heuristic method we are working on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "811f4d4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T17:38:49.125767Z",
     "start_time": "2022-09-23T17:38:49.122256Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:06.805285Z",
     "iopub.status.busy": "2023-03-20T21:42:06.804142Z",
     "iopub.status.idle": "2023-03-20T21:42:06.813561Z",
     "shell.execute_reply": "2023-03-20T21:42:06.812344Z",
     "shell.execute_reply.started": "2023-03-20T21:42:06.805223Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ds.unbatch().take(10).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0897f",
   "metadata": {},
   "source": [
    "### `Dataset` iteration\n",
    "\n",
    "`Dataset`s are python iterables! Let's have a look at that now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77787736",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T19:02:31.033418Z",
     "start_time": "2022-09-23T19:02:31.030379Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:08.023568Z",
     "iopub.status.busy": "2023-03-20T21:42:08.022825Z",
     "iopub.status.idle": "2023-03-20T21:42:08.031735Z",
     "shell.execute_reply": "2023-03-20T21:42:08.030513Z",
     "shell.execute_reply.started": "2023-03-20T21:42:08.023512Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3003042853411838\n",
      "0.24456574035714496\n",
      "0.9984157078557113\n",
      "0.9592794840556339\n",
      "0.8656060126343419\n",
      "0.5492409303870289\n",
      "0.20690550626773896\n",
      "0.29400305581091024\n",
      "0.8367717639755184\n",
      "0.9956552021875325\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the unbatched data and show the 0,0'th element\n",
    "for el in data_ds.unbatch().take(10):\n",
    "    print(el[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a84534",
   "metadata": {},
   "source": [
    "### `Dataset` - Supervised, Unsupervised, and Indexed datasets\n",
    "\n",
    "`Dataset` supports various types of datasets currently, the most common difference between two `Dataset`s is whether they are supervised, and whether they are indexed. Let's consider non-indexed `Dataset`s first. Unsupervised, the `Dataset` contains the 'input' elements (the value we would pass to a model) or `X` elements, and the target elements or the `Y` elements. When retrieving values from the `Dataset`, you will get a tuple like (`X`, `Y`). If such a `Dataset` is unsupervised, then you will just get the `X` value. If the dataset is batched, then `X` and `Y` will be batches as well. If the dataset is indexed, the returned data will be nested in another tuple whose first element is the index data. For unsupervised data, it will look like this: `(I, X)` where `I` is the index for the element. And for supervised data, it will look like this: `(I, (X, Y))`. Thus to get the index for indexed data, we access `el[0]` where `el` is the data element.\n",
    "\n",
    "Let's have a look at how this works for a supervised dataset. We'll create a new supervised dataset by generating random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "217140d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T20:18:00.860309Z",
     "start_time": "2022-09-23T20:18:00.831661Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:09.231079Z",
     "iopub.status.busy": "2023-03-20T21:42:09.230511Z",
     "iopub.status.idle": "2023-03-20T21:42:09.276738Z",
     "shell.execute_reply": "2023-03-20T21:42:09.275421Z",
     "shell.execute_reply.started": "2023-03-20T21:42:09.231027Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the new supervised data\n",
    "num_examples = 10000\n",
    "x_shape = (20, 5)\n",
    "num_classes = 5\n",
    "x_data = np.random.random((num_examples,)+x_shape)\n",
    "y_data = np.random.choice(list(range(num_classes)), size=(num_examples,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4bca635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T20:25:48.643065Z",
     "start_time": "2022-09-23T20:25:48.640540Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:09.540197Z",
     "iopub.status.busy": "2023-03-20T21:42:09.539625Z",
     "iopub.status.idle": "2023-03-20T21:42:09.546355Z",
     "shell.execute_reply": "2023-03-20T21:42:09.545096Z",
     "shell.execute_reply.started": "2023-03-20T21:42:09.540144Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create NumpyDataset\n",
    "ds = NumpyDataset((x_data, y_data), supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333f032",
   "metadata": {},
   "source": [
    "We can verify this data has the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d264220",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T20:42:05.772757Z",
     "start_time": "2022-09-23T20:42:05.768338Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:10.346790Z",
     "iopub.status.busy": "2023-03-20T21:42:10.346218Z",
     "iopub.status.idle": "2023-03-20T21:42:10.354418Z",
     "shell.execute_reply": "2023-03-20T21:42:10.352837Z",
     "shell.execute_reply.started": "2023-03-20T21:42:10.346737Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (10000, 20, 5)\n",
      "Y shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X shape: {ds.peek()[0].shape}\")\n",
    "print(f\"Y shape: {ds.peek()[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5859cc4c",
   "metadata": {},
   "source": [
    "We can index this dataset using the `as_indexed` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b228c01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T20:44:40.854776Z",
     "start_time": "2022-09-23T20:44:40.849828Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:11.510700Z",
     "iopub.status.busy": "2023-03-20T21:42:11.510130Z",
     "iopub.status.idle": "2023-03-20T21:42:11.522301Z",
     "shell.execute_reply": "2023-03-20T21:42:11.520972Z",
     "shell.execute_reply.started": "2023-03-20T21:42:11.510648Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I shape: (10000,)\n",
      "X shape: (10000, 20, 5)\n",
      "Y shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "indexed_ds = ds.as_indexed()\n",
    "first_el = indexed_ds.peek()\n",
    "print(f\"I shape: {first_el[0].shape}\")\n",
    "print(f\"X shape: {first_el[1][0].shape}\")\n",
    "print(f\"Y shape: {first_el[1][1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba11b0",
   "metadata": {},
   "source": [
    "By default, `as_indexed` counts examples starting from 0, so we can look at the first 10 elements index of the data using `unbatch`, `take`, and the `index` method which returns an iterable which just gives the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c7197b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T20:50:24.294001Z",
     "start_time": "2022-09-23T20:50:24.289940Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:12.472695Z",
     "iopub.status.busy": "2023-03-20T21:42:12.472158Z",
     "iopub.status.idle": "2023-03-20T21:42:12.483242Z",
     "shell.execute_reply": "2023-03-20T21:42:12.481952Z",
     "shell.execute_reply.started": "2023-03-20T21:42:12.472646Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for el in indexed_ds.unbatch().take(10).index():\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da8ecee",
   "metadata": {},
   "source": [
    "We can also remove the supervised data from `ds` using the `as_unsupervised` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e96bd8a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T20:57:54.408990Z",
     "start_time": "2022-09-23T20:57:54.405283Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:13.510525Z",
     "iopub.status.busy": "2023-03-20T21:42:13.509961Z",
     "iopub.status.idle": "2023-03-20T21:42:13.519788Z",
     "shell.execute_reply": "2023-03-20T21:42:13.518532Z",
     "shell.execute_reply.started": "2023-03-20T21:42:13.510473Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 20, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.as_not_supervised().peek().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4503ea66",
   "metadata": {},
   "source": [
    "### `Dataset` - `map`, `map_el`, `apply`, `apply_X`, and `apply_Y`\n",
    "\n",
    "`Dataset` supports mapping of a function across all elements of the dataset. This is useful for applying transformations to the dataset, and other components of DRYML operating on datasets use these methods in their implementations.\n",
    "\n",
    "* `map` applies a given function to all content in the `Dataset`.\n",
    "* `apply` applies a given function to all `X` and `Y` content in the `Dataset`.\n",
    "* `apply_X` applies a given function only to the `X` dataset in a `Dataset`.\n",
    "* `apply_Y` applies a given function only to the `Y` dataset in a `Dataset`.\n",
    "* `map_el` is a special function. You should have noticed at this point that all elements yielded by the dataset are within a tuple or by themselves. So `map_el` applies a function to each piece of primitive data within a collection such as tuple and applies a given function to every primative element of data within supported collections. So if the element is `(d1, (d2, d3))`, this will give `(f(d1), (f(d2), f(d3))`.\n",
    "\n",
    "Let's try `apply_X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afcd5be4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T21:09:14.945663Z",
     "start_time": "2022-09-23T21:09:14.938046Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:15.336797Z",
     "iopub.status.busy": "2023-03-20T21:42:15.336556Z",
     "iopub.status.idle": "2023-03-20T21:42:15.346064Z",
     "shell.execute_reply": "2023-03-20T21:42:15.345496Z",
     "shell.execute_reply.started": "2023-03-20T21:42:15.336777Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We apply a function to the Dataset\n",
    "el_sq = ds.apply_X(lambda x: x**2).peek()[0]\n",
    "# We can check that the function was applied with an assert\n",
    "el = ds.peek()[0]\n",
    "assert np.all(el_sq == el**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e146b",
   "metadata": {},
   "source": [
    "## DRYML `Dataset` - ML Framework transformations\n",
    "\n",
    "DRYML `Dataset` comes with a special new power. `Datasets` can implement transformations to datasets in other ML frameworks! For example, tensorflow!. the `NumpyDataset` class supports the method `tf` which creates a new `tf.data.Dataset` wrapped in a `TFDataset` (Implementing the DRYML `Dataset` API for TensorFlow) which contains the data from the original `NumpyDataset`! This is very useful for moving data into tensorflow tensors, to be used in tensorflow models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4950e91a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T21:16:56.688242Z",
     "start_time": "2022-09-23T21:16:56.633111Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:16.415701Z",
     "iopub.status.busy": "2023-03-20T21:42:16.415135Z",
     "iopub.status.idle": "2023-03-20T21:42:31.308800Z",
     "shell.execute_reply": "2023-03-20T21:42:31.307976Z",
     "shell.execute_reply.started": "2023-03-20T21:42:16.415648Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 16:42:30.652171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6392 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1\n",
      "2023-03-20 16:42:30.684924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 7362 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_ds = ds.tf()\n",
    "# We can now peek at the first element of the new dataset and see it's type.\n",
    "type(tf_ds.peek()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f0b5a",
   "metadata": {},
   "source": [
    "We can see it's now a tensorflow `EagerTensor` (it may also be just a `Tensor`).\n",
    "\n",
    "Other transformations may exist as well. the `torch` method turns the tensor into a pytorch tensor, and `numpy` turns it back into a `NumpyDataset`. Be aware, that these types of transformations currently come with large performance hits, and there is a benefit to staying within a single ML ecosystem, however this ability makes exploring new algorithms much simpler, as we don't have to re-program our data source right away and can take advantage of data input pipelines already built in other frameworks when testing new frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b4f6f73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T21:23:03.549569Z",
     "start_time": "2022-09-23T21:23:02.627235Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:33.494764Z",
     "iopub.status.busy": "2023-03-20T21:42:33.493921Z",
     "iopub.status.idle": "2023-03-20T21:42:36.991270Z",
     "shell.execute_reply": "2023-03-20T21:42:36.990399Z",
     "shell.execute_reply.started": "2023-03-20T21:42:33.494712Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data0/matthew/Software/NCSA/DRYML/venv_dryml_dev/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for pytorch\n",
    "type(ds.torch().peek()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17f16e74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T21:24:20.917895Z",
     "start_time": "2022-09-23T21:24:20.864654Z"
    },
    "execution": {
     "iopub.execute_input": "2023-03-20T21:42:36.993696Z",
     "iopub.status.busy": "2023-03-20T21:42:36.993048Z",
     "iopub.status.idle": "2023-03-20T21:42:37.055605Z",
     "shell.execute_reply": "2023-03-20T21:42:37.054967Z",
     "shell.execute_reply.started": "2023-03-20T21:42:36.993656Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can go back to numpy!\n",
    "type(ds.tf().numpy().peek()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095ed18",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "Like other components of DRYML, `Dataset`s can be used outside of `DRYML`. `Dataset` is great for inspecting and bridging existing datasets between different frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6833217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenCE 1.6.1 DRYML Dev [conda env:.conda-opence-v1.6.1-drymldev]",
   "language": "python",
   "name": "conda-env-.conda-opence-v1.6.1-drymldev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
